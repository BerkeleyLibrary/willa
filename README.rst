====================
  README for Willa
====================

:authors: UC Berkeley Library IT
:status: In development
:copyright: Â© The Regents of the University of California.  MIT license.


Introduction
============

This repository contains the implementation of a proof-of-concept OHC chatbot
prototype.  This prototype is written in Python.



Setup
=====

A local development environment can be set up by running::

    pip install -e '.[dev]'

You will need to set up an ``.env`` file for configuration.  An example is
provided in ``env.example``.  Details of the configuration keys available
follow later in this document.

You will also need to populate a LanceDB instance with some documents.  You
can run the ETL pipeline by running::

    python3 -m willa.etl.pipeline

You will need to ensure that your ``DEFAULT_STORAGE_DIR`` is set properly in
``.env`` and that there is at least one downloaded file in it.  You can
download an oral history from TIND using its TIND ID using the fetcher::

    python3 -m willa.etl.fetcher -t tind_id

where ``tind_id`` is the TIND ID.



Linting & Testing locally
==========================
To run the tests, you can use the following command::

    python -m unittest

To run linting::

    python -m pylint willa

To run mypy::

    python -m mypy willa

Deployment
==========

The chatbot service is deployed via Docker Compose.  You can set up a similar
environment by running::

    docker compose build --pull
    docker compose run prisma migrate deploy
    docker compose up -d



Configuration
=============

The following keys are available for configuration in the ``.env`` file:

``TIND_API_KEY``
    The API key to use for connecting to TIND.

``TIND_API_URL``
    The URL to use for connecting to TIND.  Should end in ``/api/v1``.

``DEFAULT_STORAGE_DIR``
    The default directory to store files retrieved from TIND.

``PROMPT_TEMPLATE``
    The inital prompt for the chatbot. defaults to ``prompt_templates/initial_prompt.txt``

``RUN_OLLAMA_TESTS``
    Set to ``true`` to run the Ollama tests.  Should only be set if Ollama is running.

``OLLAMA_URL``
    Set to the instance of Ollama to use for the Web interface.
    Defaults to ``http://localhost:11434``; you may want ``http://ollama:11434`` for Docker runs.

``CHAT_MODEL``
    The model used by the Web interface in Ollama.  Defaults to ``gemma3n:e4b``.

``CHAT_TEMPERATURE``
    Defines the "temperature" (creativeness) of the LLM.  Defaults to ``0.5``.

``POSTGRES_USER``
    The Postgres username.  Defaults to ``root``.

``POSTGRES_PASSWORD``   
    The Postgres password for POSTGRES_USER.  Defaults to ``root``.

``POSTGRES_DB``   
    The name of the database for the app.  Defaults to ``willa``.

``POSTGRES_PORT``   
    The Postgres port.  Defaults to ``5432``.

``CALNET_ENV``
    Determines which CalNet CAS environment is used for authentication.
    Valid values are ``test`` or ``prod``; if not specified, ``test`` will be used.

``CALNET_OIDC_CLIENT_ID``, ``CALNET_OIDC_CLIENT_SECRET``
    OAuth client authentication for CalNet OIDC provider.
    Make sure you are using the correct environment; test credentials do not work on the prod env.
    These credentials are kept in credential storage and must be kept secret.

``CHAINLIT_AUTH_SECRET``
    The authentication secret used by Chainlit.
    This value is generated by running ``chainlit create-secret`` and must be kept secret.

``LANCEDB_URI``
    The URI to use to connect to LanceDB.
    Note that LanceDB uses a special syntax for the URI as described in `their documentation`_.
    You probably want either ``/tmp/lancedb-path`` or ``s3://bucket/path``.

.. _`their documentation`:: https://lancedb.github.io/lancedb/guides/storage/

``AWS_ENDPOINT``, ``AWS_DEFAULT_REGION``
    The endpoint and region to use for LanceDB's S3 storage backend.
    Note: This environment variable is managed by LanceDB, not Willa.
